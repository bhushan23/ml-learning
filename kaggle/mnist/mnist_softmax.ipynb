{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf # Tensorflow for softmax and running the kernel\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "training_data = pd.read_csv(\"../input/train.csv\")\ntesting_data = pd.read_csv(\"../input/test.csv\")\nprint(training_data.head())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class Dataset(object):\n    def __init__(self, data):\n        self.rows = len(data.values)\n        self.images = data.iloc[:,1:].values\n        self.images = self.images.astype(np.float32)\n        self.images = np.multiply(self.images, 1.0 / 255.0)\n        self.labels = np.array([np.array([int(i == l) for i in range(10)]) for l in data.iloc[:,0].values]) #one-hot\n        self.index_in_epoch = 0\n        self.epoch = 0\n    def next_batch(self, batch_size):\n        start = self.index_in_epoch\n        self.index_in_epoch += batch_size\n        if self.index_in_epoch > self.rows:\n            self.epoch += 1\n            perm = np.arange(self.rows)\n            np.random.shuffle(perm)\n            self.images = self.images[perm]\n            self.labels = self.labels[perm]\n            #next epoch\n            start = 0\n            self.index_in_epoch = batch_size\n        end = self.index_in_epoch\n        return self.images[start:end] , self.labels[start:end]\n    \n    ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "x = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x, W) + b)\ny_ = tf.placeholder(tf.float32, [None, 10])\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_data = Dataset(training_data.iloc[0:37000])\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\nfor i in range(1000):\n    batch_xs, batch_ys = train_data.next_batch(100)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nvalidate_data = Dataset(training_data.iloc[37000:])\nprint(sess.run(accuracy, feed_dict={x: validate_data.images, y_: validate_data.labels}))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nplt.imshow(validate_data.images[0].reshape(28,28)) \nprint(np.argmax(sess.run(y, feed_dict={x:validate_data.images[:1]})))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test_images = testing_data.values.astype(np.float32)\ntest_images = np.multiply(test_images, 1.0 / 255.0)\n\npredictions = sess.run(y, feed_dict={x:test_images})\npredictions = [np.argmax(p) for p in predictions]\n\nresult = pd.DataFrame({'ImageId': range(1,len(predictions)+1), 'Label': predictions})\nresult.to_csv('result.csv', index=False, encoding='utf-8')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}